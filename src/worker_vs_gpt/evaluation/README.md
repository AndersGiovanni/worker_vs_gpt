# LLM Self Evaluation

This folder contains the code to evaluate the LLM model on the self-evaluation task. The code is part of my (Viktor Due Pedersen) master level research project with Luca Maria Aiello, Anders Giovanni Møller and Arianna Pera as supervisors.

- [LLM Self Evaluation](#llm-self-evaluation)
  - [The task at hand](#the-task-at-hand)
    - [Description of the ten emotions](#description-of-the-ten-emotions)
  - [Experimental setup](#experimental-setup)
    - [Generating the datasets](#generating-the-datasets)
    - [LLM Evaluation](#llm-evaluation)

## The task at hand

Through out the rest of this repository you can find the code to create the datasets that is the foundation of this task. The datasets `data/ten-dim/balanced_{gpt-4,llama-70b}_augmented_full.json` contain curated data of pairs of texts. Each entry contain a:

- h_text: The *source* text belonging to the `target`.
- `target`: The emotion label of the `h_text`.
- augmented_h_text: A LLM received `h_text` and the given `target` as prompt and was asked to generate texts containing the same emotion.

### Description of the ten emotions

- *knowledge*: Exchange of ideas or information,
- *power*: Having power over the behavior and outcomes of another,
- *respect*: Conferring status, appreciation, gratitude, or admiration upon another,
- *trust*: Will of relying on the actions or judgments of another,
- *social_support*: Giving emotional or practical aid and companionship,
- *similarity_identity*: Shared interests, motivations, outlooks or Shared sense of belonging to the same community or group,
- *fun*: Experiencing leisure, laughter, and joy,
- *conflict*: Contrast or diverging views,
- *neutral*: neutral communication

## Experimental setup

Both llama 70 b and gpt-4 are evaluated on the same data. Each dataset is generated by either llama 70 b or gpt-4. This means that the data generated by llama 70 b is evaluated both by gpt-4 and llama 70 b, and the data generated by gpt-4 is evaluated both by gpt-4 and llama 70 b. This allows us to compare the performance of the two models both on their own data and on the data generated by the other model.

### Generating the datasets

The datasets are based on subsample of 100 random text pairs from each emotion. Note that generating new data will result in different subsets of the data, since the data is randomly sampled.
Generating the datasets are done with:

```bash
python src/worker_vs_gpt/evaluation/generate_datasets.py --N 100 --model llama

python src/worker_vs_gpt/evaluation/generate_datasets.py --N 100 --model gpt
```

Results are saved like this:

```
evaluation
└── subsets
    └── label_to_other_label
        ├── gpt
        └── llama
```

### LLM Evaluation

Running LLM evaluation on the datasets are done with:

```bash
python src/worker_vs_gpt/evaluation/llama_evaluation.py
python src/worker_vs_gpt/evaluation/gpt_evaluation.py
```

Results are saved like this:

```
evaluation
└── subset_results
    └── label_to_other_label
        ├── gpt-subset
        │   ├── gpt
        │   └── llama
        └── llama-subset
            ├── gpt
            └── llama
```

Evaluating and combining the results is done by running:

```bash
python src/worker_vs_gpt/evaluation/evaluate_performance.py
```

Results are saved like this:

```
evaluation
└── assets
    ├── gpt-subset
    │   ├── gpt.csv
    │   ├── label_to_other_label.png
    │   └── llama.csv
    └── llama-subset
        ├── gpt.csv
        ├── label_to_other_label.png
        └── llama.csv
``````

<!-- ## Evaluation subset

The counts of each `target` in `data/ten-dim/balanced_gpt-4_augmented.json` are as so:

| target              | Unique source texts | Total Augmented texts | Augmentations per source text |
| ------------------- | ------------------- | --------------------- | ----------------------------- |
| similarity_identity |                     |                       |                               |
| neutral             |                     |                       |                               |
| conflict            |                     |                       |                               |
| social_support      |                     |                       |                               |
| respect             |                     |                       |                               |
| knowledge           |                     |                       |                               |
| fun                 |                     |                       |                               |
| power               |                     |                       |                               |
| trust               |                     |                       |                               |

The counts of each `target` in `data/ten-dim/balanced_llama-2-70b_augmented.json` are as so:

| target              | Unique source texts | Total Augmented texts | Augmentations per source text |
| ------------------- | ------------------- | --------------------- | ----------------------------- |
| similarity_identity |                     |                       |                               |
| neutral             |                     |                       |                               |
| conflict            |                     |                       |                               |
| social_support      |                     |                       |                               |
| respect             |                     |                       |                               |
| knowledge           |                     |                       |                               |
| fun                 |                     |                       |                               |
| power               |                     |                       |                               |
| trust               |                     |                       |                               |

Clearly, some of the emotions are underrepresented, but have the same amount of augmented texts. -->